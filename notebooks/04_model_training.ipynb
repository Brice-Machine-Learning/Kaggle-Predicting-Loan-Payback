{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kaggle: Predict Loan Payback ‚Äî Model Training\n",
    "\n",
    "**Notebook:** `04_model_training.ipynb`\n",
    "**Author:** Brice Nelson\n",
    "**Organization:** Kaggle Series | Brice Machine Learning Projects\n",
    "**Date Created:** November 16, 2025\n",
    "**Last Updated:** November 19, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Purpose\n",
    "\n",
    "This notebook initiates the **modeling phase** for the *Predict Loan Payback* competition.\n",
    "\n",
    "After completing data cleaning and feature engineering in previous notebooks, we now transition into selecting, training, evaluating, and comparing machine-learning models capable of predicting whether a borrower will repay the loan.\n",
    "\n",
    "This step turns the carefully prepared dataset into an **actionable predictive system**.\n",
    "\n",
    "### **Objectives**\n",
    "1. Load feature-engineered train/test datasets from `/data/processed/`.\n",
    "2. Define the target variable and feature matrix.\n",
    "3. Train baseline models to establish initial performance benchmarks.\n",
    "4. Evaluate models using appropriate metrics (AUC, accuracy, precision/recall, etc.).\n",
    "5. Compare multiple algorithms and select the strongest candidate(s).\n",
    "6. Export predictions for Kaggle submission.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Model Training Roadmap\n",
    "\n",
    "The modeling plan for this notebook includes:\n",
    "\n",
    "### **1. Baseline Models**\n",
    "- Logistic Regression (regularized)\n",
    "- Decision Tree (simple depth-limited version)\n",
    "\n",
    "Purpose: establish ‚Äúfloor‚Äù performance quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Core Machine Learning Models**\n",
    "- Random Forest\n",
    "- Gradient Boosting (e.g., XGBoost or LightGBM)\n",
    "- Extra Trees Classifier\n",
    "- Support Vector Machine (if practical)\n",
    "\n",
    "These will form the backbone of your model comparison phase.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Hyperparameter Tuning**\n",
    "- RandomizedSearchCV for broad sweeps\n",
    "- GridSearchCV for refining top models\n",
    "- Evaluation via stratified cross-validation\n",
    "- Tracking overfitting by comparing train vs. validation scores\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model Evaluation Metrics**\n",
    "Depending on competition scoring:\n",
    "\n",
    "- **ROC AUC** (typical for binary classification)\n",
    "- **Accuracy**\n",
    "- **Precision / Recall**\n",
    "- **Confusion matrix**\n",
    "- **Calibration curves** (optional but useful for loan risk)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Prediction & Export**\n",
    "- Predict on the processed test dataset\n",
    "- Format output to match Kaggle‚Äôs expected submission CSV\n",
    "- Save to `/data/submissions/`\n",
    "\n",
    "---\n",
    "\n",
    "## üì• Load Feature-Engineered Data\n",
    "\n",
    "This notebook begins by importing:\n",
    "\n",
    "- `../data/processed/loan_train_features.csv`\n",
    "- `../data/processed/loan_test_features.csv`\n",
    "\n",
    "(or whichever filenames you created in the feature engineering notebook)\n",
    "\n",
    "These will be used to construct the feature matrix `X` and target vector `y` for training and validation.\n"
   ],
   "id": "ae5190c6c19d903b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T06:01:59.680193Z",
     "start_time": "2025-11-21T06:01:59.671826Z"
    }
   },
   "source": [
    "import os\n",
    "import optuna\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Processed Data",
   "id": "3c158bfae076fb89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:20:47.534652Z",
     "start_time": "2025-11-21T04:20:39.023303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loan_train_features = pd.read_csv(\"../data/processed/loan_train_features.csv\")\n",
    "loan_test_features = pd.read_csv(\"../data/processed/loan_test_features.csv\")"
   ],
   "id": "c9b833795dca7537",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 1: Define Features and Target\n",
    "\n",
    "With the feature-engineered datasets loaded, the next step is to construct:\n",
    "\n",
    "- **X_train** ‚Üí Feature matrix\n",
    "- **y_train** ‚Üí Target vector (`loan_payed_back`)\n",
    "- **X_test** ‚Üí Feature matrix for Kaggle submission\n",
    "\n",
    "This section will:\n",
    "1. Separate predictors from the target column.\n",
    "2. Confirm dataset shapes and check for any remaining inconsistencies.\n",
    "3. Prepare the data for model training and baseline evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Step 2: Baseline Models\n",
    "\n",
    "Before diving into advanced algorithms, we start with simple baseline models to:\n",
    "\n",
    "- Establish a performance benchmark\n",
    "- Verify that our preprocessing is correct\n",
    "- Catch issues like data leakage or extreme imbalance early\n",
    "\n",
    "The baseline models we will train:\n",
    "\n",
    "### **1. Logistic Regression (Regularized)**\n",
    "A reliable, interpretable starting point for binary classification.\n",
    "\n",
    "### **2. Decision Tree (Depth-Limited)**\n",
    "Helps visualize splitting patterns and provides an early non-linear alternative.\n",
    "\n",
    "We‚Äôll evaluate each using:\n",
    "\n",
    "- ROC-AUC\n",
    "- Accuracy\n",
    "- Precision / Recall\n",
    "- Confusion matrix\n",
    "\n",
    "This gives us a solid ‚Äúfloor‚Äù before moving into more powerful ensemble methods.\n",
    "\n",
    "---\n"
   ],
   "id": "77e842b11b856c2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:20:47.722211Z",
     "start_time": "2025-11-21T04:20:47.670651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------\n",
    "# Loan Features Head()\n",
    "# ----------------\n",
    "print(f'Loan Train Feature:\\n{loan_train_features.head()}')\n",
    "print(f'Loan Test Featurs: \\n{loan_test_features.head()}')"
   ],
   "id": "474ef9e1da18142c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Train Feature:\n",
      "   id  annual_income  debt_to_income_ratio  credit_score  loan_amount  \\\n",
      "0   0      -0.705461             -0.535135      0.993849    -1.803484   \n",
      "1   1      -0.977248              0.660668     -0.810394    -1.505401   \n",
      "2   2       0.050689             -0.345556      0.236067     0.286558   \n",
      "3   3      -0.050687             -0.812211     -2.668764    -1.492497   \n",
      "4   4      -0.850388             -0.987206     -0.287163    -0.409421   \n",
      "\n",
      "   interest_rate  loan_paid_back     grade  subgrade  gender_Female  ...  \\\n",
      "0       0.653899             1.0 -0.401966  0.008691            1.0  ...   \n",
      "1       0.280571             0.0  0.613154  0.008691            0.0  ...   \n",
      "2      -1.292385             1.0 -0.401966  1.434819            0.0  ...   \n",
      "3       1.863482             1.0  2.643393 -1.417436            1.0  ...   \n",
      "4      -1.068388             1.0  0.613154 -1.417436            0.0  ...   \n",
      "\n",
      "   grade_x_loan_purpose_Car  grade_x_loan_purpose_Debt consolidation  \\\n",
      "0                      -0.0                                -0.000000   \n",
      "1                       0.0                                 0.613154   \n",
      "2                      -0.0                                -0.401966   \n",
      "3                       0.0                                 2.643393   \n",
      "4                       0.0                                 0.000000   \n",
      "\n",
      "   grade_x_loan_purpose_Education  grade_x_loan_purpose_Home  \\\n",
      "0                            -0.0                       -0.0   \n",
      "1                             0.0                        0.0   \n",
      "2                            -0.0                       -0.0   \n",
      "3                             0.0                        0.0   \n",
      "4                             0.0                        0.0   \n",
      "\n",
      "   grade_x_loan_purpose_Medical  grade_x_loan_purpose_Other  \\\n",
      "0                          -0.0                   -0.401966   \n",
      "1                           0.0                    0.000000   \n",
      "2                          -0.0                   -0.000000   \n",
      "3                           0.0                    0.000000   \n",
      "4                           0.0                    0.613154   \n",
      "\n",
      "   grade_x_loan_purpose_Vacation  annual_income_qt  loan_amount_qt  \\\n",
      "0                           -0.0         -0.575202       -1.876317   \n",
      "1                            0.0         -1.112297       -1.496519   \n",
      "2                           -0.0          0.138346        0.336037   \n",
      "3                            0.0          0.008228       -1.475347   \n",
      "4                            0.0         -0.834436       -0.427928   \n",
      "\n",
      "   debt_to_income_ratio_qt  \n",
      "0                -0.337506  \n",
      "1                 0.773140  \n",
      "2                 0.055229  \n",
      "3                -0.939747  \n",
      "4                -1.374878  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "Loan Test Featurs: \n",
      "       id  annual_income  debt_to_income_ratio  credit_score  loan_amount  \\\n",
      "0  593994      -0.727434             -1.045538     -0.990818    -0.513804   \n",
      "1  593995      -0.059367             -0.403888      0.921679     0.068137   \n",
      "2  593996       0.252422              3.591842     -1.261455    -1.620421   \n",
      "3  593997      -0.844850             -0.155978     -0.178909    -1.219370   \n",
      "4  593998      -0.862632             -0.578883      0.127812     0.386426   \n",
      "\n",
      "   interest_rate     grade  subgrade  gender_Female  gender_Male  ...  \\\n",
      "0       1.181536  0.613154  1.434819            1.0          0.0  ...   \n",
      "1       0.245727 -0.401966 -1.417436            1.0          0.0  ...   \n",
      "2       0.464746  0.613154 -1.417436            0.0          1.0  ...   \n",
      "3      -1.386961 -0.401966  0.008691            1.0          0.0  ...   \n",
      "4       0.220838 -0.401966 -1.417436            1.0          0.0  ...   \n",
      "\n",
      "   grade_x_loan_purpose_Car  grade_x_loan_purpose_Debt consolidation  \\\n",
      "0                       0.0                                 0.000000   \n",
      "1                      -0.0                                -0.000000   \n",
      "2                       0.0                                 0.613154   \n",
      "3                      -0.0                                -0.401966   \n",
      "4                      -0.0                                -0.000000   \n",
      "\n",
      "   grade_x_loan_purpose_Education  grade_x_loan_purpose_Home  \\\n",
      "0                             0.0                        0.0   \n",
      "1                            -0.0                       -0.0   \n",
      "2                             0.0                        0.0   \n",
      "3                            -0.0                       -0.0   \n",
      "4                            -0.0                       -0.0   \n",
      "\n",
      "   grade_x_loan_purpose_Medical  grade_x_loan_purpose_Other  \\\n",
      "0                           0.0                    0.613154   \n",
      "1                          -0.0                   -0.401966   \n",
      "2                           0.0                    0.000000   \n",
      "3                          -0.0                   -0.000000   \n",
      "4                          -0.0                   -0.000000   \n",
      "\n",
      "   grade_x_loan_purpose_Vacation  annual_income_qt  loan_amount_qt  \\\n",
      "0                            0.0         -0.622048       -0.537028   \n",
      "1                           -0.0         -0.006272        0.091341   \n",
      "2                            0.0          0.444699       -1.621009   \n",
      "3                           -0.0         -0.823488       -1.194990   \n",
      "4                           -0.0         -0.874121        0.461271   \n",
      "\n",
      "   debt_to_income_ratio_qt  \n",
      "0                -1.562721  \n",
      "1                -0.089192  \n",
      "2                 2.505309  \n",
      "3                 0.238089  \n",
      "4                -0.401995  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:20:48.056982Z",
     "start_time": "2025-11-21T04:20:47.908463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------\n",
    "# Loan Features Info()\n",
    "# ----------------\n",
    "\n",
    "print('Loan Train Features:\\n', loan_train_features.info())\n",
    "print('Loan Test Features: \\n', loan_test_features.info())"
   ],
   "id": "2c4a0a4ab90892ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593994 entries, 0 to 593993\n",
      "Data columns (total 53 columns):\n",
      " #   Column                                   Non-Null Count   Dtype  \n",
      "---  ------                                   --------------   -----  \n",
      " 0   id                                       593994 non-null  int64  \n",
      " 1   annual_income                            593994 non-null  float64\n",
      " 2   debt_to_income_ratio                     593994 non-null  float64\n",
      " 3   credit_score                             593994 non-null  float64\n",
      " 4   loan_amount                              593994 non-null  float64\n",
      " 5   interest_rate                            593994 non-null  float64\n",
      " 6   loan_paid_back                           593994 non-null  float64\n",
      " 7   grade                                    593994 non-null  float64\n",
      " 8   subgrade                                 593994 non-null  float64\n",
      " 9   gender_Female                            593994 non-null  float64\n",
      " 10  gender_Male                              593994 non-null  float64\n",
      " 11  gender_Other                             593994 non-null  float64\n",
      " 12  marital_status_Divorced                  593994 non-null  float64\n",
      " 13  marital_status_Married                   593994 non-null  float64\n",
      " 14  marital_status_Single                    593994 non-null  float64\n",
      " 15  marital_status_Widowed                   593994 non-null  float64\n",
      " 16  education_level_Bachelor's               593994 non-null  float64\n",
      " 17  education_level_High School              593994 non-null  float64\n",
      " 18  education_level_Master's                 593994 non-null  float64\n",
      " 19  education_level_Other                    593994 non-null  float64\n",
      " 20  education_level_PhD                      593994 non-null  float64\n",
      " 21  employment_status_Employed               593994 non-null  float64\n",
      " 22  employment_status_Retired                593994 non-null  float64\n",
      " 23  employment_status_Self-employed          593994 non-null  float64\n",
      " 24  employment_status_Student                593994 non-null  float64\n",
      " 25  employment_status_Unemployed             593994 non-null  float64\n",
      " 26  loan_purpose_Business                    593994 non-null  float64\n",
      " 27  loan_purpose_Car                         593994 non-null  float64\n",
      " 28  loan_purpose_Debt consolidation          593994 non-null  float64\n",
      " 29  loan_purpose_Education                   593994 non-null  float64\n",
      " 30  loan_purpose_Home                        593994 non-null  float64\n",
      " 31  loan_purpose_Medical                     593994 non-null  float64\n",
      " 32  loan_purpose_Other                       593994 non-null  float64\n",
      " 33  loan_purpose_Vacation                    593994 non-null  float64\n",
      " 34  loan_to_income                           593994 non-null  float64\n",
      " 35  high_dti_flag_0                          593994 non-null  bool   \n",
      " 36  high_dti_flag_1                          593994 non-null  bool   \n",
      " 37  credit_bucket_very_low                   593994 non-null  bool   \n",
      " 38  credit_bucket_low                        593994 non-null  bool   \n",
      " 39  credit_bucket_medium                     593994 non-null  bool   \n",
      " 40  credit_bucket_high                       593994 non-null  bool   \n",
      " 41  credit_bucket_very_high                  593994 non-null  bool   \n",
      " 42  grade_x_loan_purpose_Business            593994 non-null  float64\n",
      " 43  grade_x_loan_purpose_Car                 593994 non-null  float64\n",
      " 44  grade_x_loan_purpose_Debt consolidation  593994 non-null  float64\n",
      " 45  grade_x_loan_purpose_Education           593994 non-null  float64\n",
      " 46  grade_x_loan_purpose_Home                593994 non-null  float64\n",
      " 47  grade_x_loan_purpose_Medical             593994 non-null  float64\n",
      " 48  grade_x_loan_purpose_Other               593994 non-null  float64\n",
      " 49  grade_x_loan_purpose_Vacation            593994 non-null  float64\n",
      " 50  annual_income_qt                         593994 non-null  float64\n",
      " 51  loan_amount_qt                           593994 non-null  float64\n",
      " 52  debt_to_income_ratio_qt                  593994 non-null  float64\n",
      "dtypes: bool(7), float64(45), int64(1)\n",
      "memory usage: 212.4 MB\n",
      "Loan Train Features:\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 254569 entries, 0 to 254568\n",
      "Data columns (total 52 columns):\n",
      " #   Column                                   Non-Null Count   Dtype  \n",
      "---  ------                                   --------------   -----  \n",
      " 0   id                                       254569 non-null  int64  \n",
      " 1   annual_income                            254569 non-null  float64\n",
      " 2   debt_to_income_ratio                     254569 non-null  float64\n",
      " 3   credit_score                             254569 non-null  float64\n",
      " 4   loan_amount                              254569 non-null  float64\n",
      " 5   interest_rate                            254569 non-null  float64\n",
      " 6   grade                                    254569 non-null  float64\n",
      " 7   subgrade                                 254569 non-null  float64\n",
      " 8   gender_Female                            254569 non-null  float64\n",
      " 9   gender_Male                              254569 non-null  float64\n",
      " 10  gender_Other                             254569 non-null  float64\n",
      " 11  marital_status_Divorced                  254569 non-null  float64\n",
      " 12  marital_status_Married                   254569 non-null  float64\n",
      " 13  marital_status_Single                    254569 non-null  float64\n",
      " 14  marital_status_Widowed                   254569 non-null  float64\n",
      " 15  education_level_Bachelor's               254569 non-null  float64\n",
      " 16  education_level_High School              254569 non-null  float64\n",
      " 17  education_level_Master's                 254569 non-null  float64\n",
      " 18  education_level_Other                    254569 non-null  float64\n",
      " 19  education_level_PhD                      254569 non-null  float64\n",
      " 20  employment_status_Employed               254569 non-null  float64\n",
      " 21  employment_status_Retired                254569 non-null  float64\n",
      " 22  employment_status_Self-employed          254569 non-null  float64\n",
      " 23  employment_status_Student                254569 non-null  float64\n",
      " 24  employment_status_Unemployed             254569 non-null  float64\n",
      " 25  loan_purpose_Business                    254569 non-null  float64\n",
      " 26  loan_purpose_Car                         254569 non-null  float64\n",
      " 27  loan_purpose_Debt consolidation          254569 non-null  float64\n",
      " 28  loan_purpose_Education                   254569 non-null  float64\n",
      " 29  loan_purpose_Home                        254569 non-null  float64\n",
      " 30  loan_purpose_Medical                     254569 non-null  float64\n",
      " 31  loan_purpose_Other                       254569 non-null  float64\n",
      " 32  loan_purpose_Vacation                    254569 non-null  float64\n",
      " 33  loan_to_income                           254569 non-null  float64\n",
      " 34  high_dti_flag_0                          254569 non-null  bool   \n",
      " 35  high_dti_flag_1                          254569 non-null  bool   \n",
      " 36  credit_bucket_very_low                   254569 non-null  bool   \n",
      " 37  credit_bucket_low                        254569 non-null  bool   \n",
      " 38  credit_bucket_medium                     254569 non-null  bool   \n",
      " 39  credit_bucket_high                       254569 non-null  bool   \n",
      " 40  credit_bucket_very_high                  254569 non-null  bool   \n",
      " 41  grade_x_loan_purpose_Business            254569 non-null  float64\n",
      " 42  grade_x_loan_purpose_Car                 254569 non-null  float64\n",
      " 43  grade_x_loan_purpose_Debt consolidation  254569 non-null  float64\n",
      " 44  grade_x_loan_purpose_Education           254569 non-null  float64\n",
      " 45  grade_x_loan_purpose_Home                254569 non-null  float64\n",
      " 46  grade_x_loan_purpose_Medical             254569 non-null  float64\n",
      " 47  grade_x_loan_purpose_Other               254569 non-null  float64\n",
      " 48  grade_x_loan_purpose_Vacation            254569 non-null  float64\n",
      " 49  annual_income_qt                         254569 non-null  float64\n",
      " 50  loan_amount_qt                           254569 non-null  float64\n",
      " 51  debt_to_income_ratio_qt                  254569 non-null  float64\n",
      "dtypes: bool(7), float64(44), int64(1)\n",
      "memory usage: 89.1 MB\n",
      "Loan Test Features: \n",
      " None\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:20:48.412859Z",
     "start_time": "2025-11-21T04:20:48.119639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Step 1: Define Features (X) and Target (y)\n",
    "# -----------------------------------------------\n",
    "\n",
    "# The target column from the training set\n",
    "target_col = \"loan_paid_back\"\n",
    "\n",
    "# Feature matrix and target for training\n",
    "X_train = loan_train_features.drop(columns=[target_col])\n",
    "y_train = loan_train_features[target_col]\n",
    "\n",
    "# Test set has no target column ‚Äî that's correct\n",
    "X_test = loan_test_features.copy()\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n"
   ],
   "id": "cd4e72b62793a6ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (593994, 52)\n",
      "y_train: (593994,)\n",
      "X_test: (254569, 52)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:20:49.456226Z",
     "start_time": "2025-11-21T04:20:48.496721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Step 2: Train/Validate Split\n",
    "# -----------------------------------------------\n",
    "\n",
    "X_train_split, X_valid, y_train_split, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Train split:\", X_train_split.shape)\n",
    "print(\"Valid split:\", X_valid.shape)\n"
   ],
   "id": "3a4568d4e65a19ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: (475195, 52)\n",
      "Valid split: (118799, 52)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:22:14.082796Z",
     "start_time": "2025-11-21T04:20:49.592468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Step 3: Baseline Logistics Regression\n",
    "# -----------------------------------------------\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = log_reg.predict(X_valid)\n",
    "y_prob_lr = log_reg.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_lr = roc_auc_score(y_valid, y_prob_lr)\n",
    "\n",
    "print(f\"ROC-AUC (Logistic Regression): {roc_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred_lr))\n"
   ],
   "id": "3c942bf268f92051",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brice-nelson/miniconda3/envs/predict_loan_payback/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Logistic Regression): 0.9058\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.78      0.68     23900\n",
      "         1.0       0.94      0.87      0.90     94899\n",
      "\n",
      "    accuracy                           0.85    118799\n",
      "   macro avg       0.77      0.82      0.79    118799\n",
      "weighted avg       0.87      0.85      0.86    118799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üìä Baseline Model Results: Logistic Regression\n",
    "\n",
    "The first model trained‚Äîregularized Logistic Regression‚Äîserves as the baseline for evaluating all future models. Despite being a simple linear classifier, it produced **surprisingly strong results**, indicating that the engineered features contain significant predictive signal.\n",
    "\n",
    "### **üîé Performance Summary**\n",
    "- **ROC-AUC:** 0.9058\n",
    "- **Recall (Class 1 ‚Äì Paid Back):** 0.87\n",
    "- **Precision (Class 1 ‚Äì Paid Back):** 0.94\n",
    "- **Recall (Class 0 ‚Äì Not Paid Back):** 0.78\n",
    "- **Overall Accuracy:** 0.85\n",
    "\n",
    "### **üìà Interpretation**\n",
    "- An ROC-AUC above **0.90** from a baseline model is exceptional for a credit-risk dataset and confirms that the feature engineering phase was effective.\n",
    "- High **precision** for repaid loans (1.0) and good **recall** for non-paid loans (0.0) indicate that the model is capturing both sides of the classification boundary.\n",
    "- The class imbalance (loan_paid_back = 1 is more common) is handled well by the model, especially with `class_weight=\"balanced\"`.\n",
    "- The convergence warning from `lbfgs` is expected due to the dataset size and feature heterogeneity; it does not invalidate the results.\n",
    "\n",
    "This strong baseline establishes a **performance floor** that subsequent models must exceed.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Next Steps: Advancing Beyond the Baseline\n",
    "\n",
    "With the baseline complete, the next phase focuses on more expressive non-linear models. The dataset includes ratios, interaction terms, and many one-hot encoded features‚Äîconditions under which tree-based ensemble methods typically outperform linear models.\n",
    "\n",
    "### **üöÄ Upcoming Modeling Steps**\n",
    "\n",
    "#### **1. Train Non-Linear Baseline Models**\n",
    "- **Random Forest Classifier**\n",
    "  Establishes an early non-linear benchmark.\n",
    "\n",
    "- **Gradient Boosting Models:**\n",
    "  - XGBoost\n",
    "  - LightGBM\n",
    "  - CatBoost\n",
    "  These models are well-known for dominating tabular data competitions.\n",
    "\n",
    "#### **2. Compare Performance Using Key Metrics**\n",
    "- ROC-AUC\n",
    "- Precision/Recall\n",
    "- F1 Score\n",
    "- PR-AUC (important for imbalanced datasets)\n",
    "\n",
    "Evaluate all models on the same validation split for a fair comparison.\n",
    "\n",
    "#### **3. Hyperparameter Tuning**\n",
    "Once a top-performing algorithm is identified, apply:\n",
    "- **Optuna** (recommended for speed/efficiency), or\n",
    "- **GridSearchCV / RandomizedSearchCV**\n",
    "\n",
    "Goal: improve generalization and push leaderboard performance higher.\n",
    "\n",
    "#### **4. Save the Best Model**\n",
    "Export final tuned model using:\n",
    "- `joblib.dump(model, \"model.pkl\")`\n",
    "\n",
    "This ensures reproducibility and supports prediction generation later.\n",
    "\n",
    "#### **5. Generate Kaggle Submission**\n",
    "Use the selected model to create predictions on `X_test` and save them as:\n",
    "- `/data/submissions/submission_<date>.csv`\n",
    "\n",
    "---\n",
    "\n",
    "This roadmap transitions the project from a strong baseline into competitive modeling territory and prepares the foundation for leaderboard submissions.\n"
   ],
   "id": "8bf0f057c4c8541b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üå≤ Random Forest Classifier ‚Äî Non-Linear Baseline\n",
    "\n",
    "With the Logistic Regression baseline established, the next step is to introduce a more expressive non-linear model. Random Forests are ensemble methods that combine many decision trees trained on bootstrapped samples of the data. They naturally capture:\n",
    "\n",
    "- Non-linear relationships\n",
    "- Interaction effects\n",
    "- Hierarchical decision boundaries\n",
    "- Feature importance signals\n",
    "\n",
    "Given our dataset includes engineered ratios, one-hot encodings, and interaction terms, Random Forests provide a strong early benchmark for tree-based models.\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ Goals of This Model**\n",
    "1. Establish a non-linear baseline model.\n",
    "2. Compare its performance against Logistic Regression.\n",
    "3. Evaluate improvements in capturing complex relationships.\n",
    "4. Examine feature importance as an interpretability step.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß Model Configuration**\n",
    "For this first pass, we will use a moderate-sized forest:\n",
    "\n",
    "- `n_estimators = 300`\n",
    "- `max_depth = None` (allow deep trees)\n",
    "- `min_samples_leaf = 2`\n",
    "- `max_features = \"sqrt\"`\n",
    "- `class_weight = \"balanced\"` (handles class imbalance)\n",
    "\n",
    "This configuration keeps training efficient while still leveraging the power of ensemble methods.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìà Evaluation Metrics**\n",
    "As with Logistic Regression, we will evaluate using:\n",
    "\n",
    "- ROC-AUC\n",
    "- Precision / Recall\n",
    "- F1-score\n",
    "- Classification Report\n",
    "\n",
    "These metrics help determine whether non-linearity materially improves model performance.\n",
    "\n",
    "---\n"
   ],
   "id": "3797c99a1cc17eb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:29:16.217937Z",
     "start_time": "2025-11-21T04:22:14.328669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Random Forest Classifier ‚Äî Non-Linear Baseline\n",
    "# -----------------------------------------------\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf.predict(X_valid)\n",
    "y_prob_rf = rf.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_rf = roc_auc_score(y_valid, y_prob_rf)\n",
    "\n",
    "print(f\"ROC-AUC (Random Forest): {roc_rf:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred_rf))\n"
   ],
   "id": "b9bd15c74334ef24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Random Forest): 0.9129\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.64      0.72     23900\n",
      "         1.0       0.91      0.97      0.94     94899\n",
      "\n",
      "    accuracy                           0.90    118799\n",
      "   macro avg       0.87      0.80      0.83    118799\n",
      "weighted avg       0.90      0.90      0.90    118799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:29:16.499457Z",
     "start_time": "2025-11-21T04:29:16.470593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Initialize model comparison table if not defined\n",
    "# -----------------------------------------------\n",
    "if \"results\" not in globals():\n",
    "    results = pd.DataFrame(columns=[\"Model\", \"ROC-AUC\"])\n",
    "\n",
    "# If Logistic Regression results exist, add them here\n",
    "# Only add if roc_lr is defined\n",
    "if \"roc_lr\" in globals():\n",
    "    results.loc[len(results)] = [\"Logistic Regression\", roc_lr]\n",
    "\n",
    "# Only add Decision Tree if it exists\n",
    "if \"roc_dt\" in globals():\n",
    "    results.loc[len(results)] = [\"Decision Tree\", roc_dt]\n",
    "\n",
    "results\n"
   ],
   "id": "89aaa2c5fddc37c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 Model  ROC-AUC\n",
       "0  Logistic Regression  0.90583"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.90583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:29:16.604307Z",
     "start_time": "2025-11-21T04:29:16.572024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Append Random Forest Results\n",
    "# --------------------------\n",
    "\n",
    "results.loc[len(results)] = [\"Random Forest\", roc_rf]\n",
    "results.sort_values(\"ROC-AUC\", ascending=False)\n",
    "\n"
   ],
   "id": "7d3853ba6fe0f73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 Model   ROC-AUC\n",
       "1        Random Forest  0.912929\n",
       "0  Logistic Regression  0.905830"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.912929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üå≥ Extra Trees Classifier ‚Äî Enhanced Ensemble Baseline\n",
    "\n",
    "Following the Random Forest model, the next ensemble to evaluate is the **Extra Trees Classifier** (Extremely Randomized Trees). While similar to Random Forests, this model adds additional randomness by:\n",
    "\n",
    "- Selecting split thresholds **at random**, rather than by searching for the best possible split\n",
    "- Reducing variance and overfitting in many cases\n",
    "- Training faster due to fewer split evaluations\n",
    "\n",
    "This makes Extra Trees a valuable comparison point and often a strong performer on high-dimensional tabular data.\n",
    "\n",
    "### **üéØ Goals**\n",
    "- Evaluate the performance of Extra Trees compared to Random Forest and Logistic Regression\n",
    "- Identify whether additional randomness improves generalization\n",
    "- Capture non-linear and interaction effects that linear models cannot\n",
    "\n",
    "### **üìà Evaluation Metrics**\n",
    "We will evaluate the model using:\n",
    "- ROC-AUC\n",
    "- Precision / Recall\n",
    "- F1-score\n",
    "- Classification Report\n",
    "\n",
    "The goal is to determine whether Extra Trees surpasses Random Forest or provides complementary insights.\n",
    "\n",
    "---\n"
   ],
   "id": "9e373d0ea7507848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:49:54.645912Z",
     "start_time": "2025-11-21T04:44:58.034299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Extra Trees Classifier\n",
    "# -----------------------------------------------\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_pred_et = et.predict(X_valid)\n",
    "y_prob_et = et.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_et = roc_auc_score(y_valid, y_prob_et)\n",
    "\n",
    "print(f\"ROC-AUC (Extra Trees): {roc_et:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred_et))\n"
   ],
   "id": "f68e38934bd6b3e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Extra Trees): 0.9101\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.71      0.72     23900\n",
      "         1.0       0.93      0.93      0.93     94899\n",
      "\n",
      "    accuracy                           0.89    118799\n",
      "   macro avg       0.83      0.82      0.82    118799\n",
      "weighted avg       0.89      0.89      0.89    118799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T04:53:24.214292Z",
     "start_time": "2025-11-21T04:53:24.191920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results.loc[len(results)] = [\"Extra Trees Classifier\", roc_et]\n",
    "results.sort_values(\"ROC-AUC\", ascending=False)\n"
   ],
   "id": "f5cda6bc137f12f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    Model   ROC-AUC\n",
       "1           Random Forest  0.912929\n",
       "2  Extra Trees Classifier  0.910075\n",
       "0     Logistic Regression  0.905830"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.912929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.910075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üö´ Why Support Vector Machines Are Not Used\n",
    "\n",
    "Although Support Vector Machines (SVMs) are powerful classifiers, especially for smaller or medium-sized datasets, they are **not practical for this project** due to the size and structure of the data. The loan dataset contains nearly **600,000 rows** and over **50 engineered features**, which creates several performance challenges for SVMs.\n",
    "\n",
    "### **1. Computational Complexity**\n",
    "SVMs scale between:\n",
    "\n",
    "- **O(n¬≤)** and **O(n¬≥)** in memory and compute\n",
    "- where *n* is the number of samples (‚âà 600k here)\n",
    "\n",
    "This makes SVMs extremely slow‚Äîeven for linear kernels‚Äîand often unusable for datasets of this size.\n",
    "\n",
    "### **2. Kernel SVMs Are Completely Infeasible**\n",
    "A kernelized SVM requires computing an **n √ó n kernel matrix**, which would be:\n",
    "\n",
    ">600,000 √ó 600,000 ‚Üí 360,000,000,000 entries\n",
    "\n",
    "Even storing this matrix is impossible on typical hardware.\n",
    "\n",
    "### **3. Long Training Times on Limited Hardware**\n",
    "On a laptop CPU:\n",
    "\n",
    "- **LinearSVC** can take 20‚Äì60 minutes\n",
    "- **RBF/Polynomial SVM** can take **hours**, or fail due to memory exhaustion\n",
    "\n",
    "Given that Random Forest completed in ~7 minutes, an SVM would be dramatically slower with no performance gain.\n",
    "\n",
    "### **4. Limited Benefit for Tabular Data**\n",
    "For large, structured datasets with:\n",
    "\n",
    "- numeric features\n",
    "- one-hot encodings\n",
    "- interaction terms\n",
    "- engineered ratios\n",
    "\n",
    "tree-based ensemble methods (Random Forest, XGBoost, LightGBM, CatBoost) consistently outperform SVMs. They model non-linear relationships and feature interactions far more efficiently.\n",
    "\n",
    "### **5. No Probabilities Without Extra Cost**\n",
    "SVMs do not natively produce predicted probabilities.\n",
    "To compute AUC or PR-AUC properly, models require:\n",
    "\n",
    "- **Platt scaling** or\n",
    "- **cross-validation calibration**\n",
    "\n",
    "These steps further increase runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìå Summary: Why SVM Was Skipped**\n",
    "\n",
    "| Reason | Impact |\n",
    "|-------|--------|\n",
    "| Very poor scaling on 600k rows | Training becomes impractically long |\n",
    "| Kernel matrix is impossible to compute | Kernel SVM is not feasible |\n",
    "| High RAM usage | Likely to crash on laptop |\n",
    "| Not competitive for tabular data | RF/GBM models outperform SVM |\n",
    "| Extra work for probability outputs | Slower evaluation pipeline |\n",
    "\n",
    "Given these limitations, SVMs do not align with the project‚Äôs efficiency, hardware constraints, or performance targets.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Next Step: Gradient Boosting with LightGBM\n",
    "\n",
    "LightGBM is designed for:\n",
    "\n",
    "- **large-scale tabular data**\n",
    "- **high-dimensional feature spaces**\n",
    "- **fast training on CPUs**\n",
    "- **strong leaderboard performance**\n",
    "\n",
    "It will form the backbone of the next modeling phase.\n",
    "\n",
    "---\n"
   ],
   "id": "7c852c736bad949b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## ‚ö° LightGBM ‚Äî Gradient Boosting Optimized for Tabular Data\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) is one of the most powerful algorithms for structured/tabular datasets. It is specifically engineered for **speed**, **scalability**, and **high predictive accuracy**, making it ideal for this competition.\n",
    "\n",
    "Unlike Random Forests or Extra Trees, which average many deep trees, LightGBM builds trees **sequentially**, with each new tree correcting the errors of the previous one (gradient boosting). It also uses advanced optimizations such as:\n",
    "\n",
    "- **Histogram-based splitting** (much faster than exact splits)\n",
    "- **Leaf-wise tree growth** (increases accuracy)\n",
    "- **Efficient handling of high-dimensional data**\n",
    "- **Native support for missing values**\n",
    "\n",
    "Given the size of this dataset (~600k rows √ó 50 features), LightGBM is particularly well suited.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Goals for This Model**\n",
    "- Establish the first gradient boosting baseline\n",
    "- Compare performance against Random Forest and Extra Trees\n",
    "- Determine whether boosting provides a significant accuracy lift\n",
    "- Build a foundation for later hyperparameter tuning (Optuna or GridSearch)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Model Configuration (Laptop-Optimized)**\n",
    "\n",
    "To ensure LightGBM trains quickly even on lower-power hardware (e.g., a laptop):\n",
    "\n",
    "- `n_estimators = 300`\n",
    "- `learning_rate = 0.05`\n",
    "- `num_leaves = 31`\n",
    "- `max_depth = -1` (no forced limit; but leaves small enough to avoid overfitting)\n",
    "- `class_weight = \"balanced\"`\n",
    "- `n_jobs = -1`\n",
    "\n",
    "This configuration provides competitive performance without long compute time.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Evaluation Metrics**\n",
    "We will again evaluate:\n",
    "\n",
    "- ROC-AUC\n",
    "- Precision / Recall\n",
    "- F1-score\n",
    "- Classification Report\n",
    "\n",
    "This will help determine whether LightGBM surpasses the tree ensemble baselines.\n",
    "\n",
    "---\n"
   ],
   "id": "237fbb710b65b7c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T05:09:51.527270Z",
     "start_time": "2025-11-21T05:09:37.457063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# LightGBM Classifier ‚Äî Gradient Boosting Baseline\n",
    "# -----------------------------------------------\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgb = lgbm.predict(X_valid)\n",
    "y_prob_lgb = lgbm.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_lgb = roc_auc_score(y_valid, y_prob_lgb)\n",
    "\n",
    "print(f\"ROC-AUC (LightGBM): {roc_lgb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred_lgb))\n"
   ],
   "id": "61976fff61a5ddc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 379595, number of negative: 95600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2674\n",
      "[LightGBM] [Info] Number of data points in the train set: 475195, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "ROC-AUC (LightGBM): 0.9201\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.79      0.70     23900\n",
      "         1.0       0.94      0.88      0.91     94899\n",
      "\n",
      "    accuracy                           0.87    118799\n",
      "   macro avg       0.79      0.84      0.81    118799\n",
      "weighted avg       0.88      0.87      0.87    118799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T05:10:05.292741Z",
     "start_time": "2025-11-21T05:10:05.271649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results.loc[len(results)] = [\"LightGBM\", roc_lgb]\n",
    "results.sort_values(\"ROC-AUC\", ascending=False)\n"
   ],
   "id": "a0304c7449e532f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    Model   ROC-AUC\n",
       "3                LightGBM  0.920053\n",
       "1           Random Forest  0.912929\n",
       "2  Extra Trees Classifier  0.910075\n",
       "0     Logistic Regression  0.905830"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.920053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.912929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.910075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üêà CatBoost ‚Äî Powerful Gradient Boosting for Tabular Data\n",
    "\n",
    "CatBoost (Categorical Boosting) is one of the strongest gradient boosting algorithms for structured/tabular datasets. It excels in scenarios with:\n",
    "\n",
    "- many engineered features\n",
    "- non-linear relationships\n",
    "- interaction terms\n",
    "- imbalanced datasets\n",
    "- one-hot encodings (even though it prefers raw categorical columns)\n",
    "\n",
    "Unlike other boosting methods, CatBoost incorporates:\n",
    "\n",
    "- **Ordered boosting**, which reduces overfitting\n",
    "- **Efficient handling of categorical patterns**\n",
    "- **Symmetric tree structures**, which improve speed and generalization\n",
    "- **Fast CPU performance**, making it ideal for laptop environments\n",
    "\n",
    "Given the size and structure of this dataset (~600k rows, 50+ engineered features), CatBoost is a natural next model in the competitive modeling phase.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Goals for This Model\n",
    "- Benchmark CatBoost against LightGBM, Random Forest, Extra Trees, and Logistic Regression\n",
    "- Evaluate whether its regularization and tree symmetry improve ROC-AUC\n",
    "- Prepare the model for potential hyperparameter tuning with Optuna\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Model Configuration (Laptop-Friendly)\n",
    "To ensure CatBoost runs efficiently on CPU:\n",
    "\n",
    "- `iterations = 300`\n",
    "- `learning_rate = 0.05`\n",
    "- `depth = 6`\n",
    "- `l2_leaf_reg = 3`\n",
    "- `loss_function = \"Logloss\"`\n",
    "- `eval_metric = \"AUC\"`\n",
    "- `class_weights = {0: w0, 1: w1}` (CatBoost handles class imbalance well)\n",
    "\n",
    "This setup provides strong early performance without overheating the system.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Evaluation Metrics\n",
    "As before, we evaluate:\n",
    "\n",
    "- ROC-AUC\n",
    "- Precision / Recall\n",
    "- F1-score\n",
    "- Classification Report\n",
    "\n",
    "The goal is to determine whether CatBoost surpasses LightGBM‚Äôs baseline.\n",
    "\n",
    "---\n"
   ],
   "id": "96fe21fc03c0a5e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T05:25:37.522069Z",
     "start_time": "2025-11-21T05:24:59.483625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# CatBoost Classifier ‚Äî Gradient Boosting Baseline\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Compute class imbalance for CatBoost weights\n",
    "# (Because it's ratio-based, not \"balanced\" like sklearn)\n",
    "pos_weight = (y_train_split == 0).sum() / (y_train_split == 1).sum()\n",
    "neg_weight = 1\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    class_weights=[neg_weight, pos_weight],\n",
    "    verbose=50,          # Print progress every 50 iterations\n",
    "    random_seed=42,\n",
    "    task_type=\"CPU\"\n",
    ")\n",
    "\n",
    "cat_model.fit(\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    eval_set=(X_valid, y_valid),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_pred_cat = cat_model.predict(X_valid)\n",
    "y_prob_cat = cat_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_cat = roc_auc_score(y_valid, y_prob_cat)\n",
    "\n",
    "print(f\"ROC-AUC (CatBoost): {roc_cat:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred_cat))\n"
   ],
   "id": "d53a0eb4702d17ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8972484\tbest: 0.8972484 (0)\ttotal: 164ms\tremaining: 49.1s\n",
      "50:\ttest: 0.9134485\tbest: 0.9134485 (50)\ttotal: 7.76s\tremaining: 37.9s\n",
      "100:\ttest: 0.9149204\tbest: 0.9149204 (100)\ttotal: 12.9s\tremaining: 25.5s\n",
      "150:\ttest: 0.9158680\tbest: 0.9158680 (150)\ttotal: 20.5s\tremaining: 20.3s\n",
      "200:\ttest: 0.9165813\tbest: 0.9165813 (200)\ttotal: 25.7s\tremaining: 12.7s\n",
      "250:\ttest: 0.9170486\tbest: 0.9170486 (250)\ttotal: 31.4s\tremaining: 6.13s\n",
      "299:\ttest: 0.9175801\tbest: 0.9175801 (299)\ttotal: 36.6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9175800883\n",
      "bestIteration = 299\n",
      "\n",
      "ROC-AUC (CatBoost): 0.9176\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.79      0.70     23900\n",
      "         1.0       0.94      0.88      0.91     94899\n",
      "\n",
      "    accuracy                           0.86    118799\n",
      "   macro avg       0.79      0.83      0.81    118799\n",
      "weighted avg       0.88      0.86      0.87    118799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T05:25:47.465591Z",
     "start_time": "2025-11-21T05:25:47.434292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results.loc[len(results)] = [\"CatBoost\", roc_cat]\n",
    "results.sort_values(\"ROC-AUC\", ascending=False)\n"
   ],
   "id": "d6ca1d40943b5e3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    Model   ROC-AUC\n",
       "3                LightGBM  0.920053\n",
       "4                CatBoost  0.917580\n",
       "1           Random Forest  0.912929\n",
       "2  Extra Trees Classifier  0.910075\n",
       "0     Logistic Regression  0.905830"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.920053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.917580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.912929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.910075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üöÄ XGBoost ‚Äî Gradient Boosting with Robust Regularization\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is one of the most influential algorithms in modern machine learning. It dominated Kaggle competitions for years and remains a go-to choice in fintech, risk modeling, credit scoring, fraud detection, and structured/tabular ML.\n",
    "\n",
    "While LightGBM is typically faster, XGBoost offers:\n",
    "\n",
    "- Highly effective regularization (L1 + L2)\n",
    "- Strong handling of noisy or imperfect features\n",
    "- Excellent performance on large, structured datasets\n",
    "- Predictable, stable behavior under most conditions\n",
    "\n",
    "For this project, XGBoost provides a valuable comparison point alongside LightGBM and CatBoost, and completing it ensures a thorough modeling phase.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Goals for This Model\n",
    "- Benchmark XGBoost against LightGBM, CatBoost, Random Forest, and Extra Trees\n",
    "- Understand how different boosting strategies impact performance\n",
    "- Build foundational experience with XGBoost for real-world ML workflows\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Model Configuration (Laptop-Friendly)\n",
    "To avoid long training times while still capturing performance:\n",
    "\n",
    "- `n_estimators = 300`\n",
    "- `learning_rate = 0.05`\n",
    "- `max_depth = 6`\n",
    "- `subsample = 0.8`\n",
    "- `colsample_bytree = 0.8`\n",
    "- `reg_alpha = 0.0`\n",
    "- `reg_lambda = 1.0`\n",
    "- `objective = \"binary:logistic\"`\n",
    "- `eval_metric = \"auc\"`\n",
    "\n",
    "This configuration balances speed and quality for a large dataset (~600k rows).\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Evaluation Metrics\n",
    "We will evaluate the model using:\n",
    "\n",
    "- ROC-AUC\n",
    "- Precision / Recall\n",
    "- F1-score\n",
    "- Classification Report\n",
    "\n",
    "This determines whether XGBoost approaches or surpasses LightGBM's current lead.\n",
    "\n",
    "---\n"
   ],
   "id": "458e73ebe7d6fadf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T05:36:44.462417Z",
     "start_time": "2025-11-21T05:35:50.772479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# XGBoost Classifier ‚Äî Gradient Boosting Baseline\n",
    "# -----------------------------------------------\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",        # Fastest CPU method (VERY important)\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    X_train_split,\n",
    "    y_train_split,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb.predict(X_valid)\n",
    "y_prob_xgb = xgb.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_xgb = roc_auc_score(y_valid, y_prob_xgb)\n",
    "\n",
    "print(f\"ROC-AUC (XGBoost): {roc_xgb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred_xgb))\n"
   ],
   "id": "32646e0ce94e61cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (XGBoost): 0.9187\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.60      0.71     23900\n",
      "         1.0       0.91      0.98      0.94     94899\n",
      "\n",
      "    accuracy                           0.90    118799\n",
      "   macro avg       0.90      0.79      0.83    118799\n",
      "weighted avg       0.90      0.90      0.90    118799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T05:37:36.287747Z",
     "start_time": "2025-11-21T05:37:36.043907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results.loc[len(results)] = [\"XGBoost\", roc_xgb]\n",
    "results.sort_values(\"ROC-AUC\", ascending=False)\n"
   ],
   "id": "48e0d158b31f2cbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    Model   ROC-AUC\n",
       "3                LightGBM  0.920053\n",
       "5                 XGBoost  0.918702\n",
       "4                CatBoost  0.917580\n",
       "1           Random Forest  0.912929\n",
       "2  Extra Trees Classifier  0.910075\n",
       "0     Logistic Regression  0.905830"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.920053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.918702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.917580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.912929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees Classifier</td>\n",
       "      <td>0.910075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Hyperparameter Tuning ‚Äî What It Is and Why We Need It\n",
    "\n",
    "Now that all baseline models have been trained and compared, the next step is to **optimize** the top-performing algorithms. Out of all models tested so far, **LightGBM** and **XGBoost** have shown the strongest ROC-AUC scores and are the best candidates for tuning.\n",
    "\n",
    "Hyperparameter tuning is the process of systematically searching for the best settings (hyperparameters) that control how a model learns. These settings can dramatically affect:\n",
    "\n",
    "- Model accuracy\n",
    "- Overfitting vs. generalization\n",
    "- Training speed\n",
    "- Final leaderboard performance\n",
    "\n",
    "Baseline models give us a strong starting point, but they are rarely optimized for maximum AUC.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why We Use Optuna**\n",
    "Optuna is a modern hyperparameter optimization framework that uses **Bayesian optimization** and **smart search strategies** to find high-performing configurations efficiently.\n",
    "\n",
    "Compared to manual tuning or grid search:\n",
    "\n",
    "- üöÄ **Faster** (finds good configs in fewer trials)\n",
    "- üß† **Smarter** (uses previous results to guide future searches)\n",
    "- üíª **Resource-efficient** (great for laptops and limited hardware)\n",
    "- üìà **Consistently improves model AUC**\n",
    "\n",
    "For large tabular datasets like this one, Optuna is a near-essential tool for pushing model performance into the top tier (0.94‚Äì0.96 AUC range).\n",
    "\n",
    "---\n",
    "\n",
    "### **What We Will Tune**\n",
    "We will focus on the two strongest models:\n",
    "\n",
    "1. **LightGBM**\n",
    "2. **XGBoost**\n",
    "\n",
    "Key parameters that influence performance:\n",
    "\n",
    "- Number of leaves / tree depth\n",
    "- Learning rate\n",
    "- Number of boosting rounds\n",
    "- Subsample ratios\n",
    "- Feature sampling ratios\n",
    "- Regularization (L1/L2 penalties)\n",
    "- Minimum child weight / min data in leaf\n",
    "\n",
    "These control how the model grows trees and how much it generalizes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Goal of This Phase**\n",
    "The objective is to find the **best possible configuration** for the model that achieves:\n",
    "\n",
    "- Higher ROC-AUC\n",
    "- Stronger precision-recall characteristics\n",
    "- Better ranking of default risk\n",
    "- Improved stability on unseen data\n",
    "\n",
    "After tuning, we will:\n",
    "\n",
    "- Re-train the best model\n",
    "- Save it under `/models/`\n",
    "- Use it to generate final Kaggle submission predictions\n",
    "\n",
    "This marks the final stage of the modeling workflow.\n",
    "\n",
    "---\n"
   ],
   "id": "6f940d1f7d1ccedb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## üîß Hyperparameter Tuning: LightGBM + Optuna\n",
    "\n",
    "With LightGBM currently leading model performance (ROC-AUC = 0.92005), the next step is to tune its hyperparameters to push the model toward higher accuracy and better generalization.\n",
    "\n",
    "LightGBM is highly sensitive to its core hyperparameters, including:\n",
    "\n",
    "- **num_leaves**\n",
    "- **max_depth**\n",
    "- **learning_rate**\n",
    "- **subsample** and **colsample_bytree**\n",
    "- **min_child_samples**\n",
    "- **lambda_l1 / lambda_l2** (regularization)\n",
    "\n",
    "Manually tuning these would be slow and inefficient.\n",
    "Instead, we use **Optuna**, which performs:\n",
    "\n",
    "- intelligent hyperparameter search\n",
    "- guided by Bayesian optimization\n",
    "- efficient even on CPU\n",
    "- ideal for large tabular datasets\n",
    "\n",
    "The goal is to discover a configuration that significantly improves ROC-AUC over the baseline while maintaining reasonable training time.\n",
    "\n",
    "After tuning, the best LightGBM model will be retrained on the full training split and saved for later evaluation and submission.\n",
    "\n",
    "---\n"
   ],
   "id": "8bc28071e4ec7c82"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-21T06:05:05.291339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------\n",
    "# Optuna Objective Function for LightGBM\n",
    "# ------------------------------------------------------\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 60),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 60),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 5.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 5.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"verbosity\": -1     # <-- silence training output safely\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_split,\n",
    "        y_train_split,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric=\"auc\"\n",
    "    )\n",
    "\n",
    "    preds = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "\n",
    "    return auc\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# ------------------------------------------------------\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"lightgbm_opt\"\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=25,     # keep small for laptop; increase to 50‚Äì100 on desktop\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Best ROC-AUC:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ],
   "id": "c95fc30eff58900f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 01:05:05,299] A new study created in memory with name: lightgbm_opt\n",
      "Best trial: 0. Best value: 0.919935:   4%|‚ñç         | 1/25 [00:22<08:57, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 01:05:27,679] Trial 0 finished with value: 0.9199348856289009 and parameters: {'n_estimators': 355, 'learning_rate': 0.03366493246417459, 'num_leaves': 32, 'max_depth': 12, 'min_child_samples': 53, 'subsample': 0.9613508147206281, 'colsample_bytree': 0.7488672455637454, 'lambda_l1': 1.98944238703894, 'lambda_l2': 1.6577568762284183}. Best is trial 0 with value: 0.9199348856289009.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.919935:   8%|‚ñä         | 2/25 [00:41<07:51, 20.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 01:05:46,861] Trial 1 finished with value: 0.9141593493739061 and parameters: {'n_estimators': 697, 'learning_rate': 0.06810896335860102, 'num_leaves': 31, 'max_depth': 1, 'min_child_samples': 13, 'subsample': 0.9432970916613678, 'colsample_bytree': 0.8018301437845513, 'lambda_l1': 3.8164306378833746, 'lambda_l2': 1.0120548304357362}. Best is trial 0 with value: 0.9199348856289009.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4276ad0874eb99ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
